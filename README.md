# Video Generation Reference



## Papers with code

[Text To Video](https://paperswithcode.com/task/text-to-video-generation)



## followyourpose
- https://github.com/mayuelala/followyourpose
- TL;DR: We tune the text-to-image model (e.g., stable diffusion) to generate the character videos from pose and text description.

![astronout](./astronaut.gif)



## Tune-A-Video
-https://github.com/showlab/Tune-A-Video
- Given a video-text pair as input, our method, Tune-A-Video, fine-tunes a pre-trained text-to-image diffusion model for text-to-video generation.
![example](https://camo.githubusercontent.com/40b7d8416024a0b08b1766f29e8d1a90efc2deff9ed879b21cc32a7bd7498a10/68747470733a2f2f74756e6561766964656f2e6769746875622e696f2f6173736574732f7465617365722e676966)


## Text2Video-Zero

- https://github.com/picsart-ai-research/text2video-zero
Our method Text2Video-Zero enables zero-shot video generation using
(i) a textual prompt (see rows 1, 2),
(ii) a prompt combined with guidance from poses or edges (see lower right), and
(iii) Video Instruct-Pix2Pix, i.e., instruction-guided video editing (see lower left).

- Text to Video
![cat](https://github.com/Picsart-AI-Research/Text2Video-Zero/blob/main/__assets__/github/results/t2v/cat_running.gif)
"A cat is running on the grass"

- Text To Video with Edge Guidance
![edge](https://github.com/Picsart-AI-Research/Text2Video-Zero/blob/main/__assets__/github/results/edge2v/jelly_merged_with_input.gif)
"A jelly Fish"

![arcane](https://github.com/Picsart-AI-Research/Text2Video-Zero/blob/main/__assets__/github/results/canny_db/arcane_style_merged_with_input.gif)
"arcane style"

## Thin-Plate Spline Motion Model for Image Animation
![animate](https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model/raw/main/assets/vox.gif)

"talking head video" + image --> video


### Control Net

- https://github.com/lllyasviel/ControlNet#:~:text=ControlNet%20is%20a%20neural%20network,trainable%22%20one%20learns%20your%20condition.
- ControlNet is a neural network structure to control diffusion models by adding extra conditions.
- https://stablediffusionweb.com/ControlNet#demo

used by caliber?

### Inpainting

https://github.com/hitachinsk/FGT

![one](https://github.com/hitachinsk/FGT/blob/master/materials/demo_o/bmx-bumps_o.gif)
![two](https://github.com/hitachinsk/FGT/blob/master/materials/demo_p/bbp.gif)


### Frame Interpolation
- https://github.com/megvii-research/ECCV2022-RIFE
![girl with ball](https://github.com/megvii-research/ECCV2022-RIFE/blob/main/demo/D2_slomo_clipped.gif)

![girl with ball2](https://github.com/megvii-research/ECCV2022-RIFE/blob/main/demo/D2_slomo_clipped.gif)

### Kaliber



### Talking heads:

- https://www.synthesia.io/
- https://www.deepbrain.io/
- https://elai.io/
- https://app.heygen.com/


## Image tools

![Instruct-Pix2Pix](https://www.timothybrooks.com/instruct-pix2pix/)
![teaser](https://instruct-pix2pix.timothybrooks.com/teaser.jpg)

### Stability Ai

- https://beta.dreamstudio.ai/
## Utils

### Audio editing
https://www.descript.com/podcasting

![MMPose](https://github.com/open-mmlab/mmpose)


### Look into:

https://invideo.io/?ref=yti0mte
https://www.gling.ai/#how-it-works
